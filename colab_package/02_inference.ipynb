{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d110b8b",
   "metadata": {},
   "source": [
    "# Industrial Safety Equipment Detection - Inference Notebook\n",
    "\n",
    "This notebook demonstrates how to use the trained YOLOv8 model for detecting industrial safety equipment (FireExtinguisher, ToolBox, OxygenTank) in images.\n",
    "\n",
    "## Features:\n",
    "- Load trained YOLOv8 model\n",
    "- Run inference on single images or batch processing\n",
    "- Visualize detection results\n",
    "- Export results in various formats\n",
    "- Performance benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cc9249",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9bc424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages in Colab\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check if running in Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install ultralytics\n",
    "    !pip install opencv-python\n",
    "    !pip install pillow\n",
    "    print(\"Packages installed successfully!\")\n",
    "else:\n",
    "    print(\"Not running in Colab - assuming packages are already installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efc95f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import time\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from ultralytics import YOLO\n",
    "from IPython.display import display, Image as IPImage\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571e2c1d",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2478ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'model_path': 'runs/train/industrial_safety_detection/weights/best.pt',  # Path to trained model\n",
    "    'conf_threshold': 0.25,\n",
    "    'iou_threshold': 0.45,\n",
    "    'device': 'auto',  # 'auto', 'cpu', 'cuda', or specific GPU ID\n",
    "    'class_names': ['FireExtinguisher', 'ToolBox', 'OxygenTank'],\n",
    "    'colors': [(255, 0, 0), (0, 255, 0), (0, 0, 255)],  # BGR colors for each class\n",
    "    'output_dir': 'inference_results'\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f351b295",
   "metadata": {},
   "source": [
    "## 3. Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64488261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if model exists\n",
    "model_path = CONFIG['model_path']\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"⚠ Model not found at: {model_path}\")\n",
    "    print(\"Please make sure you have trained the model first or provide the correct path.\")\n",
    "    print(\"\\nAlternatively, you can use a pretrained YOLO model:\")\n",
    "    model_path = 'yolov8n.pt'  # Use pretrained model as fallback\n",
    "    print(f\"Using pretrained model: {model_path}\")\n",
    "else:\n",
    "    print(f\"✓ Model found: {model_path}\")\n",
    "\n",
    "# Load model\n",
    "try:\n",
    "    model = YOLO(model_path)\n",
    "    print(f\"✓ Model loaded successfully\")\n",
    "    \n",
    "    # Model info\n",
    "    print(f\"\\nModel Information:\")\n",
    "    model.info()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9207324d",
   "metadata": {},
   "source": [
    "## 4. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bae4eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(image_path, results, class_names, colors, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize detection results on an image.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the input image\n",
    "        results: YOLO results object\n",
    "        class_names: List of class names\n",
    "        colors: List of colors for each class\n",
    "        save_path: Optional path to save the visualization\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    image = cv2.imread(str(image_path))\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(image_rgb)\n",
    "    \n",
    "    # Get current axes\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    # Process detections\n",
    "    if len(results) > 0 and len(results[0].boxes) > 0:\n",
    "        boxes = results[0].boxes\n",
    "        \n",
    "        for box in boxes:\n",
    "            # Get box coordinates\n",
    "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "            conf = float(box.conf[0])\n",
    "            class_id = int(box.cls[0])\n",
    "            \n",
    "            # Get class name and color\n",
    "            if class_id < len(class_names):\n",
    "                class_name = class_names[class_id]\n",
    "                color = np.array(colors[class_id]) / 255.0  # Normalize for matplotlib\n",
    "            else:\n",
    "                class_name = f\"Class_{class_id}\"\n",
    "                color = (1.0, 1.0, 1.0)  # White for unknown classes\n",
    "            \n",
    "            # Draw bounding box\n",
    "            rect = patches.Rectangle(\n",
    "                (x1, y1), x2 - x1, y2 - y1,\n",
    "                linewidth=2, edgecolor=color, facecolor='none'\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Add label\n",
    "            label = f\"{class_name}: {conf:.2f}\"\n",
    "            ax.text(\n",
    "                x1, y1 - 10, label,\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=color, alpha=0.7),\n",
    "                fontsize=10, color='white', weight='bold'\n",
    "            )\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.title(f'Industrial Safety Equipment Detection\\nImage: {Path(image_path).name}', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "        print(f\"Visualization saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def get_detection_summary(results, class_names):\n",
    "    \"\"\"\n",
    "    Get summary of detections.\n",
    "    \n",
    "    Args:\n",
    "        results: YOLO results object\n",
    "        class_names: List of class names\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with detection summary\n",
    "    \"\"\"\n",
    "    summary = {\n",
    "        'total_detections': 0,\n",
    "        'class_counts': {name: 0 for name in class_names},\n",
    "        'confidences': [],\n",
    "        'detections': []\n",
    "    }\n",
    "    \n",
    "    if len(results) > 0 and len(results[0].boxes) > 0:\n",
    "        boxes = results[0].boxes\n",
    "        summary['total_detections'] = len(boxes)\n",
    "        \n",
    "        for box in boxes:\n",
    "            conf = float(box.conf[0])\n",
    "            class_id = int(box.cls[0])\n",
    "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "            \n",
    "            if class_id < len(class_names):\n",
    "                class_name = class_names[class_id]\n",
    "                summary['class_counts'][class_name] += 1\n",
    "            else:\n",
    "                class_name = f\"Unknown_{class_id}\"\n",
    "                if class_name not in summary['class_counts']:\n",
    "                    summary['class_counts'][class_name] = 0\n",
    "                summary['class_counts'][class_name] += 1\n",
    "            \n",
    "            summary['confidences'].append(conf)\n",
    "            summary['detections'].append({\n",
    "                'class': class_name,\n",
    "                'confidence': conf,\n",
    "                'bbox': [x1, y1, x2, y2]\n",
    "            })\n",
    "    \n",
    "    return summary\n",
    "\n",
    "print(\"✓ Utility functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fca6bc",
   "metadata": {},
   "source": [
    "## 5. Single Image Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e747053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single image inference example\n",
    "def run_single_inference(image_path, model, config):\n",
    "    \"\"\"\n",
    "    Run inference on a single image.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the input image\n",
    "        model: Loaded YOLO model\n",
    "        config: Configuration dictionary\n",
    "    \"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"❌ Image not found: {image_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Running inference on: {image_path}\")\n",
    "    \n",
    "    # Record inference time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run inference\n",
    "    results = model(\n",
    "        image_path,\n",
    "        conf=config['conf_threshold'],\n",
    "        iou=config['iou_threshold'],\n",
    "        device=config['device']\n",
    "    )\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # Get detection summary\n",
    "    summary = get_detection_summary(results, config['class_names'])\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n📊 Inference Results:\")\n",
    "    print(f\"⏱ Inference time: {inference_time:.3f} seconds\")\n",
    "    print(f\"🎯 Total detections: {summary['total_detections']}\")\n",
    "    \n",
    "    if summary['total_detections'] > 0:\n",
    "        print(f\"\\n📈 Class Distribution:\")\n",
    "        for class_name, count in summary['class_counts'].items():\n",
    "            if count > 0:\n",
    "                print(f\"  {class_name}: {count}\")\n",
    "        \n",
    "        avg_conf = np.mean(summary['confidences'])\n",
    "        print(f\"\\n🎯 Average confidence: {avg_conf:.3f}\")\n",
    "        \n",
    "        print(f\"\\n📋 Detailed detections:\")\n",
    "        for i, det in enumerate(summary['detections']):\n",
    "            print(f\"  {i+1}. {det['class']}: {det['confidence']:.3f}\")\n",
    "    \n",
    "    # Visualize results\n",
    "    output_path = os.path.join(config['output_dir'], f\"result_{Path(image_path).stem}.jpg\")\n",
    "    visualize_predictions(image_path, results, config['class_names'], config['colors'], output_path)\n",
    "    \n",
    "    return results, summary\n",
    "\n",
    "# Example usage - replace with your image path\n",
    "sample_image_path = \"data/test/images/000000000.png\"  # Update this path\n",
    "\n",
    "if os.path.exists(sample_image_path):\n",
    "    results, summary = run_single_inference(sample_image_path, model, CONFIG)\n",
    "else:\n",
    "    print(f\"Sample image not found: {sample_image_path}\")\n",
    "    print(\"Please provide a valid image path in the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008a7f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload and test your own image\n",
    "# In Colab, you can use this code to upload an image:\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    print(\"Upload an image file:\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    if uploaded:\n",
    "        # Get the first uploaded file\n",
    "        filename = list(uploaded.keys())[0]\n",
    "        print(f\"\\nUploaded: {filename}\")\n",
    "        \n",
    "        # Run inference on uploaded image\n",
    "        results, summary = run_single_inference(filename, model, CONFIG)\n",
    "else:\n",
    "    # For local execution, specify your image path\n",
    "    custom_image_path = input(\"Enter path to your image (or press Enter to skip): \")\n",
    "    \n",
    "    if custom_image_path and os.path.exists(custom_image_path):\n",
    "        results, summary = run_single_inference(custom_image_path, model, CONFIG)\n",
    "    else:\n",
    "        print(\"No valid image path provided or file not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1751b8",
   "metadata": {},
   "source": [
    "## 6. Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfa5e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_inference(images_dir, model, config, max_images=10):\n",
    "    \"\"\"\n",
    "    Run inference on multiple images in a directory.\n",
    "    \n",
    "    Args:\n",
    "        images_dir: Directory containing images\n",
    "        model: Loaded YOLO model\n",
    "        config: Configuration dictionary\n",
    "        max_images: Maximum number of images to process\n",
    "    \"\"\"\n",
    "    images_dir = Path(images_dir)\n",
    "    if not images_dir.exists():\n",
    "        print(f\"❌ Directory not found: {images_dir}\")\n",
    "        return\n",
    "    \n",
    "    # Find all image files\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']\n",
    "    image_files = []\n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(list(images_dir.glob(f'*{ext}')))\n",
    "        image_files.extend(list(images_dir.glob(f'*{ext.upper()}')))\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"❌ No image files found in: {images_dir}\")\n",
    "        return\n",
    "    \n",
    "    # Limit number of images\n",
    "    image_files = image_files[:max_images]\n",
    "    print(f\"Processing {len(image_files)} images...\")\n",
    "    \n",
    "    # Batch processing results\n",
    "    batch_results = []\n",
    "    total_detections = 0\n",
    "    total_time = 0\n",
    "    class_totals = {name: 0 for name in config['class_names']}\n",
    "    \n",
    "    for i, image_path in enumerate(image_files):\n",
    "        print(f\"\\n[{i+1}/{len(image_files)}] Processing: {image_path.name}\")\n",
    "        \n",
    "        try:\n",
    "            # Run inference\n",
    "            start_time = time.time()\n",
    "            results = model(\n",
    "                str(image_path),\n",
    "                conf=config['conf_threshold'],\n",
    "                iou=config['iou_threshold'],\n",
    "                device=config['device'],\n",
    "                verbose=False\n",
    "            )\n",
    "            inference_time = time.time() - start_time\n",
    "            total_time += inference_time\n",
    "            \n",
    "            # Get summary\n",
    "            summary = get_detection_summary(results, config['class_names'])\n",
    "            total_detections += summary['total_detections']\n",
    "            \n",
    "            # Update class totals\n",
    "            for class_name, count in summary['class_counts'].items():\n",
    "                if class_name in class_totals:\n",
    "                    class_totals[class_name] += count\n",
    "            \n",
    "            # Store results\n",
    "            batch_results.append({\n",
    "                'image': image_path.name,\n",
    "                'detections': summary['total_detections'],\n",
    "                'time': inference_time,\n",
    "                'summary': summary\n",
    "            })\n",
    "            \n",
    "            print(f\"  ✓ {summary['total_detections']} detections in {inference_time:.3f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error processing {image_path.name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Print batch summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"📊 BATCH PROCESSING SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Images processed: {len(batch_results)}\")\n",
    "    print(f\"Total detections: {total_detections}\")\n",
    "    print(f\"Total time: {total_time:.2f}s\")\n",
    "    print(f\"Average time per image: {total_time/len(batch_results):.3f}s\")\n",
    "    print(f\"Average detections per image: {total_detections/len(batch_results):.2f}\")\n",
    "    \n",
    "    print(f\"\\n📈 Class Distribution:\")\n",
    "    for class_name, count in class_totals.items():\n",
    "        if count > 0:\n",
    "            print(f\"  {class_name}: {count}\")\n",
    "    \n",
    "    return batch_results\n",
    "\n",
    "# Run batch inference on test images\n",
    "test_images_dir = \"data/test/images\"\n",
    "if os.path.exists(test_images_dir):\n",
    "    batch_results = run_batch_inference(test_images_dir, model, CONFIG, max_images=5)\n",
    "else:\n",
    "    print(f\"Test images directory not found: {test_images_dir}\")\n",
    "    print(\"Please provide a directory containing images for batch processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9014c688",
   "metadata": {},
   "source": [
    "## 7. Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70cf4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model(model, config, num_runs=10, image_size=(640, 640)):\n",
    "    \"\"\"\n",
    "    Benchmark model performance.\n",
    "    \n",
    "    Args:\n",
    "        model: Loaded YOLO model\n",
    "        config: Configuration dictionary\n",
    "        num_runs: Number of benchmark runs\n",
    "        image_size: Input image size (width, height)\n",
    "    \"\"\"\n",
    "    print(f\"🚀 Benchmarking model performance...\")\n",
    "    print(f\"Runs: {num_runs}, Image size: {image_size}\")\n",
    "    \n",
    "    # Create dummy image\n",
    "    dummy_image = np.random.randint(0, 255, (image_size[1], image_size[0], 3), dtype=np.uint8)\n",
    "    \n",
    "    # Warm up\n",
    "    print(\"Warming up...\")\n",
    "    for _ in range(3):\n",
    "        _ = model(dummy_image, verbose=False)\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    for i in range(num_runs):\n",
    "        start_time = time.time()\n",
    "        _ = model(\n",
    "            dummy_image,\n",
    "            conf=config['conf_threshold'],\n",
    "            iou=config['iou_threshold'],\n",
    "            device=config['device'],\n",
    "            verbose=False\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "        \n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"  Completed {i + 1}/{num_runs} runs\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    avg_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    min_time = np.min(times)\n",
    "    max_time = np.max(times)\n",
    "    fps = 1.0 / avg_time\n",
    "    \n",
    "    print(f\"\\n📊 BENCHMARK RESULTS\")\n",
    "    print(f\"{'='*40}\")\n",
    "    print(f\"Average inference time: {avg_time:.4f}s ± {std_time:.4f}s\")\n",
    "    print(f\"Min inference time: {min_time:.4f}s\")\n",
    "    print(f\"Max inference time: {max_time:.4f}s\")\n",
    "    print(f\"Average FPS: {fps:.2f}\")\n",
    "    print(f\"Device: {config['device']}\")\n",
    "    print(f\"Image size: {image_size}\")\n",
    "    \n",
    "    # Plot timing distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(times, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    plt.xlabel('Inference Time (seconds)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Inference Time Distribution')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, len(times) + 1), times, marker='o', linewidth=2, markersize=4)\n",
    "    plt.axhline(y=avg_time, color='red', linestyle='--', label=f'Average: {avg_time:.4f}s')\n",
    "    plt.xlabel('Run Number')\n",
    "    plt.ylabel('Inference Time (seconds)')\n",
    "    plt.title('Inference Time per Run')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'avg_time': avg_time,\n",
    "        'std_time': std_time,\n",
    "        'min_time': min_time,\n",
    "        'max_time': max_time,\n",
    "        'fps': fps,\n",
    "        'times': times\n",
    "    }\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_results = benchmark_model(model, CONFIG, num_runs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad014df",
   "metadata": {},
   "source": [
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df6215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results_to_csv(batch_results, output_path=\"inference_results.csv\"):\n",
    "    \"\"\"\n",
    "    Export batch inference results to CSV.\n",
    "    \n",
    "    Args:\n",
    "        batch_results: Results from batch inference\n",
    "        output_path: Output CSV file path\n",
    "    \"\"\"\n",
    "    if not batch_results:\n",
    "        print(\"No batch results to export.\")\n",
    "        return\n",
    "    \n",
    "    # Prepare data for CSV\n",
    "    csv_data = []\n",
    "    \n",
    "    for result in batch_results:\n",
    "        base_row = {\n",
    "            'image': result['image'],\n",
    "            'total_detections': result['detections'],\n",
    "            'inference_time': result['time']\n",
    "        }\n",
    "        \n",
    "        # Add class counts\n",
    "        for class_name in CONFIG['class_names']:\n",
    "            base_row[f'{class_name}_count'] = result['summary']['class_counts'].get(class_name, 0)\n",
    "        \n",
    "        # If there are detections, add detailed info\n",
    "        if result['summary']['detections']:\n",
    "            for i, detection in enumerate(result['summary']['detections']):\n",
    "                row = base_row.copy()\n",
    "                row.update({\n",
    "                    'detection_id': i + 1,\n",
    "                    'class': detection['class'],\n",
    "                    'confidence': detection['confidence'],\n",
    "                    'bbox_x1': detection['bbox'][0],\n",
    "                    'bbox_y1': detection['bbox'][1],\n",
    "                    'bbox_x2': detection['bbox'][2],\n",
    "                    'bbox_y2': detection['bbox'][3]\n",
    "                })\n",
    "                csv_data.append(row)\n",
    "        else:\n",
    "            # No detections\n",
    "            base_row.update({\n",
    "                'detection_id': 0,\n",
    "                'class': 'None',\n",
    "                'confidence': 0.0,\n",
    "                'bbox_x1': 0,\n",
    "                'bbox_y1': 0,\n",
    "                'bbox_x2': 0,\n",
    "                'bbox_y2': 0\n",
    "            })\n",
    "            csv_data.append(base_row)\n",
    "    \n",
    "    # Create DataFrame and save\n",
    "    df = pd.DataFrame(csv_data)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"✓ Results exported to: {output_path}\")\n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Export results if batch processing was done\n",
    "if 'batch_results' in locals() and batch_results:\n",
    "    results_df = export_results_to_csv(batch_results, \n",
    "                                      os.path.join(CONFIG['output_dir'], \"inference_results.csv\"))\n",
    "else:\n",
    "    print(\"No batch results available for export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48ee442",
   "metadata": {},
   "source": [
    "## 9. Model Analysis and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1356f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model analysis\n",
    "def analyze_model_performance():\n",
    "    \"\"\"\n",
    "    Analyze model performance and provide insights.\n",
    "    \"\"\"\n",
    "    print(\"🔍 MODEL ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Basic model info\n",
    "    print(f\"Model architecture: {CONFIG['model_path']}\")\n",
    "    print(f\"Classes: {', '.join(CONFIG['class_names'])}\")\n",
    "    print(f\"Confidence threshold: {CONFIG['conf_threshold']}\")\n",
    "    print(f\"IoU threshold: {CONFIG['iou_threshold']}\")\n",
    "    \n",
    "    # Performance metrics\n",
    "    if 'benchmark_results' in locals():\n",
    "        print(f\"\\n⚡ PERFORMANCE:\")\n",
    "        print(f\"Average inference time: {benchmark_results['avg_time']:.4f}s\")\n",
    "        print(f\"Average FPS: {benchmark_results['fps']:.2f}\")\n",
    "        \n",
    "        # Performance rating\n",
    "        if benchmark_results['fps'] > 30:\n",
    "            perf_rating = \"🚀 Excellent (Real-time capable)\"\n",
    "        elif benchmark_results['fps'] > 15:\n",
    "            perf_rating = \"⚡ Good (Near real-time)\"\n",
    "        elif benchmark_results['fps'] > 5:\n",
    "            perf_rating = \"⚠ Moderate (Batch processing suitable)\"\n",
    "        else:\n",
    "            perf_rating = \"🐌 Slow (Consider optimization)\"\n",
    "        \n",
    "        print(f\"Performance rating: {perf_rating}\")\n",
    "    \n",
    "    # Detection statistics\n",
    "    if 'batch_results' in locals() and batch_results:\n",
    "        print(f\"\\n📊 DETECTION STATISTICS:\")\n",
    "        total_images = len(batch_results)\n",
    "        total_detections = sum(r['detections'] for r in batch_results)\n",
    "        images_with_detections = sum(1 for r in batch_results if r['detections'] > 0)\n",
    "        \n",
    "        print(f\"Images processed: {total_images}\")\n",
    "        print(f\"Total detections: {total_detections}\")\n",
    "        print(f\"Images with detections: {images_with_detections} ({images_with_detections/total_images*100:.1f}%)\")\n",
    "        print(f\"Average detections per image: {total_detections/total_images:.2f}\")\n",
    "        \n",
    "        if images_with_detections > 0:\n",
    "            avg_det_per_positive = total_detections / images_with_detections\n",
    "            print(f\"Average detections per positive image: {avg_det_per_positive:.2f}\")\n",
    "    \n",
    "    print(f\"\\n💡 RECOMMENDATIONS:\")\n",
    "    if 'benchmark_results' in locals():\n",
    "        if benchmark_results['fps'] < 10:\n",
    "            print(\"  • Consider using a smaller model (YOLOv8n → YOLOv8s) for better speed\")\n",
    "            print(\"  • Try reducing input image size\")\n",
    "            print(\"  • Enable half precision (FP16) if using GPU\")\n",
    "        \n",
    "        if benchmark_results['std_time'] > benchmark_results['avg_time'] * 0.1:\n",
    "            print(\"  • High variance in inference time - consider model optimization\")\n",
    "    \n",
    "    print(\"  • Monitor confidence scores - low scores may indicate need for more training data\")\n",
    "    print(\"  • Consider ensemble methods for improved accuracy\")\n",
    "    print(\"  • Implement confidence thresholding based on your use case requirements\")\n",
    "\n",
    "# Run analysis\n",
    "analyze_model_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7a1155",
   "metadata": {},
   "source": [
    "## 10. Summary and Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd397c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎯 INFERENCE SESSION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Model: {CONFIG['model_path']}\")\n",
    "print(f\"Classes: {', '.join(CONFIG['class_names'])}\")\n",
    "print(f\"Device: {CONFIG['device']}\")\n",
    "\n",
    "if 'benchmark_results' in locals():\n",
    "    print(f\"\\n⚡ Performance:\")\n",
    "    print(f\"  Average inference time: {benchmark_results['avg_time']:.4f}s\")\n",
    "    print(f\"  Average FPS: {benchmark_results['fps']:.2f}\")\n",
    "\n",
    "if 'batch_results' in locals() and batch_results:\n",
    "    total_processed = len(batch_results)\n",
    "    total_detections = sum(r['detections'] for r in batch_results)\n",
    "    print(f\"\\n📊 Batch Processing:\")\n",
    "    print(f\"  Images processed: {total_processed}\")\n",
    "    print(f\"  Total detections: {total_detections}\")\n",
    "\n",
    "print(f\"\\n📁 Output Directory: {CONFIG['output_dir']}\")\n",
    "print(f\"  - Visualization images\")\n",
    "if 'results_df' in locals():\n",
    "    print(f\"  - Results CSV file\")\n",
    "\n",
    "print(f\"\\n🚀 Next Steps:\")\n",
    "print(f\"  1. Review detection results and adjust confidence thresholds if needed\")\n",
    "print(f\"  2. Test on more diverse images to evaluate robustness\")\n",
    "print(f\"  3. Consider model optimization for deployment\")\n",
    "print(f\"  4. Implement the model in your production pipeline\")\n",
    "\n",
    "print(\"\\n✅ Inference session completed successfully!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
